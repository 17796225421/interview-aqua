1.题目描述
给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?

2.思考过程
（1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。

（2）但上述方法有一个明显问题，加载一个文件的数据需要50亿*64bytes = 320G远远大于4G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。

（3）针对上述问题，我们分治算法的思想。

step1：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,...,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计）

step2：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,...,b999)（为什么要这样做? 文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中）

所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url）

step3：因为每个hash大约300M，所以我们再可以采用（1）中的想法

所以海量数据面前要多采用分治算法，化整为零 各个击破！

------



这个问题在面试中遇到过，我简单说一下我的思路。

我是这样理解题意的：文件a，b，分别有x，y行数据，每一行是一个字符串，现在想找到a和b中重复的数据。

这个问题其实是海量数据面试中非常常见的一个场景，更细化一点的话可能会要求使用不超过多少的内存，在这里不赘述。

思路1：使用[bloom filter算法](https://www.zhihu.com/search?q=bloom+filter算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A387830719})。

如果不考虑100%准确率，那么bloom filter将是很好的选择，不了解的可以搜索或者查看[海量数据处理算法-Bloom Filter - CSDN博客](https://link.zhihu.com/?target=https%3A//blog.csdn.net/hguisu/article/details/7866173)。

步骤：

0）创建一个大小为m的[byte数组](https://www.zhihu.com/search?q=byte数组&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A387830719})（注意：m的取值和x和y中较大的有关，通常为降低错误率，m=nlg(1/E)*lge，具体证略）；

1）遍历a（假如a是较大的文件）将每一行，对这一行执行k个[哈希函数](https://www.zhihu.com/search?q=哈希函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A387830719})得到k个哈希值，然后将m数组中对应的比特位设置为1；

2）不考虑内存的情况，此时a已经全部存储到byte数组中，然后遍历b的每一行，如果这一行经过k个hash函数得到的k个[hash值](https://www.zhihu.com/search?q=hash值&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A387830719})所对应的数组都为1，那么表示该数据是重复的，否则不重复，最后取出所有重复的部分。

该算法的空间复杂度只有m大小的数组，不会动态增长，而时间复杂度主要在于(x+y)*k。

注意：bloom filter不能保证100%正确，例如k=3,x1字符串经过3个hash得到的是0,1,2，那么在0,1,2数组位置为1，而y1经过3个hash恰好也得到0,1,2，这就判定y1存在，但实际上y1可能不存在。

思路2：hash

hash的原理就不说了，这里主要使用“大而化小，分而治之”的策略。

步骤：

0）根据给定内存需求或其他限制，预先估计要划分多少个小文件，例如1千万数据可以划分为100个文件，然后尽量保证每个文件分配的数据是均匀的；

1）遍历a（假如a是较大的文件），对a的每一行做hash运算，根据hash值将该行数据映射到一个小文件a1-a100文件中；

2）a处理完成后，此时数据已经按照[hash策略](https://www.zhihu.com/search?q=hash策略&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A387830719})分布到多个小文件中。此时遍历b，做同样的[hash算法](https://www.zhihu.com/search?q=hash算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A387830719})（注意：两个字符串如果相同，那么他们经过同一hash算法得到的必然也是相等的），映射到b1-b100小文件中；

3）逐个比较<a1,b1>文件对，此时数据量够小，可以装载到hashmap中进行比对，最后得到结果。

思路3：mapreduce

既然是海量数据，那么解决方案至少要知道用mr方案。

大致思路（如有不正确请及时指出）：

0）map端主要是遍历，以每行字符串为key，value就是所属的文件，例如<string1,a>,<string1,b>,<string2,a>;

1）reduce端主要是将相同key的数据聚合到一起，value进行拼接，那么假如value中包含a和b的，就表示是重复数据，只输出这些就可以。

最后：推荐多查阅关于海量数据面试的问题，会比较有帮助。