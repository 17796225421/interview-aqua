### 2、寻找热门查询，300 万个查询字符串中统计最热门的 10 个查询。

**原题**：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个记录，请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G.
**分析**：这些查询串的重复度比较高，虽然总数是一千万，但是如果去重后，不超过三百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。
由上面第 1 题，我们知道，数据大则划为小的，例如一亿个 IP 求 Top10，可先 %1000 将 IP 分到 1000 个小文件中去，并保证一种 ip 只出现在一个文件中，再对每个小文件中的 ip 进行 hash_map 统计并按数量排序，最后归并或者最小堆依次处理每个小文件的 top10 以得到最后的结果。
但对于本题，数据规模比较小，能一次性装入内存。因为根据题目描述，虽然有一千万个 Query，但是由于重复度比较高，故去重后，事实上只有 300 万的 Query，每个 Query 大小为 255Byte，因此我们可以考虑把他们都放进内存中去（300 万个字符串假设没有重复，都是最大长度，那么最多占用内存 3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理）。
所以我们放弃分而治之/hash 映射的步骤，直接上 hash_map 统计，然后排序。So，针对此类典型的 topK 问题，采取的对策往往是：
hash_map + 堆。
**解法**：

- hash_map 统计
  先对这批海量数据预处理，具体方法是：维护一个 Key 为 Query 字串，Value 为该 Query 出现次数的 hash_map，即 hash_map(Query, value)，每次读取一个 Query，如果该字串不在 Table 中，那么加入该字串，并将 Value 值设为 1；如果该字串在 Table 中，那么将该字串的计数加 1 即可。最终我们在 O(N) 的时间复杂度内用 hash_map 完成了统计；
- 堆排序
  借助堆这个数据结构，找出 topK，时间复杂度为 N'logK。即借助堆结构，我们可以在 log 量级的时间内查找和调整/移动。因此，维护一个 K（该题目中是 10）大小的小根堆，然后遍历300万的Query，分别和根元素进行比对。所以，我们最终的时间复杂度是：O(n) + N'*O(logK)，其中，N为1000万，N'为300万。